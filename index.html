<html>
<head>
    <title>Data Mining</title>
    <link rel="stylesheet" href="style.css">
    <body>
       <center><div>design was to experiment with the option of exploiting the aggregate memory, by means
of the Shared Tree external object. The addition of the shared structure enhances the
performance of the program, whose sequential code still works in main memory. The
implementation of out-of-core and low-level parallel functionalities in the external
objects is the following step in our research about parallel languages. Merging the taskparallel approach and the data-parallel one within the same program will clearly enhance
the performance of this kind of D&C applications.</div></center>
        <br>
        <br>
        <center><h1><b>ADVANTAGES OF STRUCTURE<br>
          PARALLELISM</b></h1></center>
        <center><div>Table 1 reports some software cost measures from our experiments, which we review
to underline the qualities of the structured approach: fast code development, code
portability, and performance portability.</div></center>
        
        <h2>Development Costs and Code Expressiveness</h2>
    <div>When restructuring the existing sequential code to parallel, most of the work is
devoted to making the code modular. The amount of sequential code needed to develop
the building blocks for structured parallel applications is reported in Table 1 as
    <i>modularization</i>, separate from the true parallel code. Once modularization has been
accomplished, several prototypes for different parallel structures are usually developed
and evaluated. The skeleton description of a parallel structure is shorter, quicker to write
and far more readable than its equivalent written in MPI. As a test, starting from the same</div>
    <br>
        <center><div><i>Table 1: Software development costs for Apriori, DBSCAN and C4.5: Number of lines
and kind of code, development times, best speedup on different target machines</div></i></center>
        
        <center><img src="IMG_20240112_155622.jpg"></center>
    <div>sequential modules, we developed an MPI version of C4.5. Though it exploits simpler
solutions (Master-Slave, no pipelined communications) than the skeleton program, the
MPI code is longer, more complex and error-prone than the structured version. On the
contrary, the speed-up results showed no significant gain from the additional programming effort.</div>
    <br>
    <br>
        <h2><b>Performance</h2></b>
        <center><div>The speed-up and scale-up results of the applications we have shown are not all
breakthrough, but comparable to those of similar solutions performed with unstructured
parallel programming (e.g., MPI). The Partitioned Apriori is fully scalable with respect to
database size, like count-distribution implementations. The C4.5 prototype behaves
better than other pure task-parallel implementations. It suffers the limits of this
parallelization scheme, due to the support of external objects being incomplete. We know
of no other results about spatial clustering using our approach to the parallelization of
            cluster expansion.</div></center>
        <h2><b>Code and Performance Portability</b></h2>
        <div>Skeleton code is by definition portable over all the architectures that support the
programming environment. Since the SkIE two-level parallel compiler uses standard
compilation tools to build the final application, the intermediate code and the run-time
support of the language can exploit all the advantages of parallel communication libraries.
We can enhance the parallel support by using architecture-specific facilities when the
performance gain is valuable, but as long as the intermediate code complies with industry
standards the applications are portable to a broad set of architectures. The SMP and T3E
tests of the ARM prototype were performed this way, with no extra development time,
by compiling on the target machine the MPI and C++ code produced by SkIE. These
            results also show a good degree of performance portability.</div>

        <h1><center>CONCLUSIONS</center><h1>
<p class="sun">We have shown how a structured parallel approach can reduce the complexity of
parallel application design, and that the approach can be usefully applied to commonly
used DM algorithms. The ease of sequential to parallel conversion and the good qualities
of code reuse are valuable in the DM field, because of the need for fast prototyping
applications and implementation solutions. Performance is achieved by means of careful
design of the application parallel structure, with low-level details left to the compiler and
the parallel language support.<br>
Within the structured parallelism framework, the proposal of external objects aims
at unifying the interfaces to different data management services: in-core memory, shared
memory, local/parallel file systems, DBMS, and data transport layers. By decoupling the
algorithm structure from the details of data access, we increase the architecture independence, and we allow the language support to implement the accesses in the best way,</p>
<p class="car">according to the size of the data and the underlying software and hardware layers. These
are very important features in the perspective of merging high-performance algorithms
into DM environments for large-scale databases. Such a vision is strongly called for in
the literature; nevertheless, only sequential DM tools currently address integration
issues. On the grounds of the experiments described here with the SkIE environment, we
are designing a full support for external objects in the new structured programming
    environment, ASSIST.<br>
Several of the points we have mentioned are still open research problems. Which
levels of the implementation will exploit parallelism is one of the questions. The
development of massively parallel DBMS systems, and the progressive adoption of
parallel file system servers, will both have a profound impact on high performance DM,
with results that are not easy to foresee. We believe that high-level parallel languages
can also play an important role in the organization and coordination of Grid computational
resources into complex applications. Executing collective DM tasks over distributed
systems requires finding the right balance between result accuracy, reduction of data
movement, and balancing of the computational workload. To prevent us from having to
deal with more and more complex management details at the same time, ASSIST will
actively support Grid protocols and communication libraries.</p>
            <center<h1><b>REFERENCES</b></h1></center>
<center><div>Agrawal, R., Mannila, H., Ramakrishnan, S., Toivonen, H., & Verkamo, A.I. (1996). Fast
discovery of association rules. In U.M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, &
R. Uthurusamy (eds.), Advances in knowledge discovery and data mining, pp. 307-
328. Cambridge, MA: AAAI Press / MIT Press.<br>
Agrawal, R., & Shafer, J. (1996). Parallel mining of association rules, IEEE Transactions
on Knowledge and Data Engineering 8 (6) 962-969.<br>
Au, P., Darlington, J., Ghanem, M., Guo, Y., To, H.W., & Yang, J. (1996). Coordinating
heterogeneous parallel computation. In L. Bouge, P.Fraigniaud, A. Mignotte &
Y.Roberts (eds.), Europar ’96, Vol. 1124 of Lecture Notes in Computer Science,
Berlin: Springer-Verlag.<br>
Bertchold, S., Keim, D. A., & Kriegel, H.-P. (1996). The X-Tree: An index structure for
high-dimensional data. In Proceedings of the 22nd International Conference on
Very Large Data Bases, Morgan Kaufmann Publishers, pp. 28-39.<br>
Beyer, K., Goldstein, J., Ramakrishnan, R., & Shaft, U. (1999). When is “nearest neighbor”
meaningful? In C. Beeri, & P. Buneman (eds.), Database Theory - ICDT’99 7th
International Conference, Vol. 1540 of Lecture Notes in Computer Science, pp.<br>
217-235. Berlin: Springer-Verlag.<br>
Carletti, G. & Coppola, M. (2002). Structured parallel programming and shared projects:
Experiences in data mining classifiers. In G.R. Joubert, A. Murli, F.J. Peters, & M.
Vanneschi (Ed.), <i>Parallel Computing, Advances and Current Issues, Proceedings
of the ParCo 2001International Conference.</i> London: Imperial College Press.<br>
Cole, M. (1989). Algorithmic skeletons: Structured management of parallel computations.<br>
Research Monographs in Parallel and Distributed Computing. London: Pitman.<br>
Coppola, M. & Vanneschi, M. (2002). High-performance data mining with Skeleton-based</p></div></center>
            <br>
                structured parallel programming. In Parallel Computing, special issue on Parallel
Data Intensive Computing, 28(5), 793-813.<br>
Danelutto, M. (2001). On Skeletons and design patterns. To appear in Parallel Computing, Advances and Current Issues, Proceedings of ParCo 2001 International
Conference. London: Imperial College Press.<br>
Ester, M., Kriegel, H.-P., Sander, J., & Xu, X. (1996). A density-based algorithm for
discovering clusters in large spatial databases with noise. In E. Simoudis, J. Han
& U. Fayyad (Eds.), Proceedings of KDD ‘96, AAAI Press, pp.226-231.<br>
Fayyad, U. M., Piatetsky-Shapiro, G., Smyth, P., & Uthurusamy, R. (eds.) (1996).<br>
Advances in knowledge discovery and data mining. Cambridge, MA: AAAI Press
/ MIT Press.<br>
Freitas, A. A. & Lavington, S.H. (1998). Mining very large databases with parallel
processing. Boston, MA: Kluwer Academic Publisher.<br>
Joshi, M. V., Han, E.-H., Karypis, G., & Kumar, V. (2000). Efficient parallel algorithms for
mining associations. In M. J. Zaki & C.-T. Ho (eds.), Large-scale parallel data
mining. Vol. 1759 of Lecture Notes in Artificial Intelligence. New York: Springer.
Joshi, M. V., Karypis, G. & Kumar, V (1998). ScalParC: A new scalable and efficient parallel
classification algorithm for mining large datasets. In Proceedings of 1998 International Parallel Processing Symposium, IEEE CS Press, pp. 573-579.<br>
Maniatty, W. A. & Zaki, M. J. (2000). A requirement analysis for parallel KDD Systems.
In J. Rolim et al. (eds.) Parallel and distributed processing, Volume 1800 of Lecture
Notes in Computer Science. Berlin: Springer-Verlag.<br>
Quinlan, J. (1993). C4.5: Programs for machine learning. San Mateo, CA: Morgan
Kaufmann.<br>
Savasere, A., Omiecinski, E., & Navathe, S. (1995). An efficient algorithm for mining
association rules in large databases. In U. Dayal, P. Gray, and S. Nishio (eds.),
Proceedings of 21st International Conference on Very Large Data Bases -VLDB ’95
Zurich, pp. 432-444. San Francisco: Morgan Kaufmann.<br>
Serot, J., Ginhac, D., Chapuis, R., & Derutin, J. (2001). Fast prototyping of parallel vision
applications using functional skeletons. Machine Vision and Applications, 12,
217-290.<br>
Shafer, J., Agrawal, R., & Mehta, M. (1996). SPRINT: A scalable parallel classifier for data
mining. In Proceedings of the 22nd International Conference on Very Large Data
Bases - VLDB ’96. , Morgan Kaufmann, pp. 544-555.<br>
Skillicorn, D. B., & Talia, D. (1998). Models and languages for parallel computation. ACM
Computing Surveys. 30 (2) 123-169.<br>
Sreenivas, M.K., AlSabti, K., & Ranka, S. (2000). Parallel out-of-core decision tree
classifiers. In H. Kargupta & P. Chan (eds.), Advances in distributed and parallel
knowledge discovery. Cambridge, MA: AAAI/MIT Press.<br>
Srivastava, A., Han, E.-H., Kumar, V., & Singh, V. (1999). Parallel formulations of decisiontree classification algorithms. Data Mining and Knowledge Discovery: An International Journal, 3(3) 237-261.<br>
Vanneschi, M. (1998a). PQE2000: HPC tools for industrial applications. IEEE Concurrency:
Parallel, Distributed & Mobile Computing, 6 (4) 68-73.<br>
Vanneschi, M. (1998b). Heterogeneous HPC environments. In D. Pritchard & J. Reeve
(eds.), Euro-Par ’98 Parallel Processing, Vol. 1470 of Lecture Notes in Computer
Science. Berlin: Springer-Verlag.<br><br>
<p>Vanneschi, M. (2002). ASSIST: A programming environment for parallel and distributed
portable applications. Internal Report, ASI-PQE2000 Project, January. Submitted
for publication.<br>
Vitter, J. S, (2001). External memory algorithms and data structures: Dealing with
MASSIVE DATA. ACM Computing Surveys, 33 (2) 209-271.<br>
Xu, X., Jager, J., & Kriegel, H.-P. (1999). A fast parallel clustering algorithm for large
spatial databases. Data Mining and Knowledge Discovery: An International
Journal, 3(3) 263-290.<br>
Zaki, M. J. (2000). Scalable algorithms for association Mining. IEEE Transactions on
Knowledge and Data Engineering, 12, 372-390.<br>
Zaki, M. J. & Ho, C.-T. (2000). Large scale parallel data mining. Vol. 1759 of Lecture
Notes in Artificial Intelligence. Berlin: Springer-Verlag.</p><br>
   
      
        
    </body>
.
</html>

